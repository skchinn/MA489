---
title: "Arrest Data Chinn"
author: "Samantha Chinn"
date: "10/23/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(spacetime)
library(sp)
library(lubridate)
library(ggmap)
library(data.table)
```


The below code reads in the arrest data.  This takes time, which is annoying because nothing changes but you need the data everytime you compile this document.  Therefore, I recommend caching the results below (see Rmd file).  As long as you don't edit this R chunk, the results will load from memory which is much faster than redownloading every time.

```{r, cache=TRUE, message = FALSE, warning=FALSE}
arrests <- read_csv(file = "C:/Users/Samantha Chinn/OneDrive - West Point/AY21-1/MA489/MA489 - Spatial Temporal Statistics/Project/NYPD_Arrests_Data__Historic_.csv")
# convert the ARREST_DATE column to a Date
# this will make it easier later because you can use functions in the lubridate package
arrests <- arrests %>% 
  mutate(ARREST_DATE = mdy(ARREST_DATE))
# use only a sample of 2015 arrests for now to speed things up
set.seed(10)
arrests2015 <- arrests %>% 
  filter(year(ARREST_DATE) == 2015) %>% 
  sample_n(5000)
```
   
The below code caches the map so you don't have to download it every time. 
   
```{r, message=FALSE, cache=TRUE}      
#it's easiest if you load the map once and then cache it, so it doesn't have to 
# import it everytime
#outline of New York City
nyc <- c(left = -74.3,
         right = -73.6, 
         top = 40.95,
         bottom = 40.4)
nyc_map <- get_stamenmap(bbox = nyc, zoom = 10, type = "toner-lite")
```

Now, plot using the downloaded map.

```{r}
nyc_map %>% ggmap() +
  geom_point(aes(x = Longitude, y = Latitude), data = arrests2015)
```


So, I'm thinking we should start simpler and build out from there.  Let's consider only arrests that occured in Times Square in 2015. 

```{r, message = FALSE}
read.table(file = "http://www.isi-stats.com/isi2/data/homeprices.txt", header = T)
left = -74
right = -73.97
top = 40.76
bottom = 40.72
timessquare <- c(left = left, right = right, top = top, bottom = bottom)
#filter points in Times Square
arrests_timessquare2015 <- arrests %>% 
  filter(Latitude < top,
         Latitude > bottom,
         Longitude < right,
         Longitude > left) %>% 
  filter(year(ARREST_DATE) == 2015)
timessquare_map <- get_stamenmap(timessquare, zoom = 15, type = "toner-lite")
timessquare_map %>% ggmap() +
  geom_point(aes(x = Longitude, y = Latitude), data = arrests_timessquare2015)
```

Next, I think the best way for us to approach this data initial is to produce a grid (lattice) of points.  Then, assign each arrest to one of the grids. Then, we can complete an analysis similar to that in Lab 3.4.

```{r}
###### So each grid is 0.005 by 0.005?
lattice <- expand_grid(Longitude = seq(timessquare[1], timessquare[2], by = 0.005),
            Latitude = seq(timessquare[4], timessquare[3], by = 0.005))
#add an id to each location
lattice <- lattice %>% 
  mutate(grid = paste("L",1:n(), sep = ""))
lattice <- data.table(lattice)
#plot a map of the grid
timessquare_map %>% ggmap() +
  geom_point(aes(x = Longitude, y = Latitude), data = arrests_timessquare2015) +
  geom_point(aes(x = Longitude, y = Latitude), data = lattice, col = "red", size = 5)
```

Next, let's assign each arrest to the appropriate grid.

```{r}
#arrests locations
arrests_loc <- arrests_timessquare2015 %>% 
  select(ARREST_KEY, Longitude, Latitude) %>% 
  data.table()
#matrix of bounds
bounds <- lattice[, .(xl = Longitude - 0.0025,
                  xu = Longitude + 0.0025,
                  yl = Latitude - 0.0025,
                  yu = Latitude + 0.0025,
                  grid)]
#assign each arrest to the appropriate grid
assignments <- bounds[arrests_loc, .(ARREST_KEY,grid),
                      on = .(xl <= Longitude, xu >= Longitude,
                             yl <= Latitude, yu >= Latitude)]
assignments <- as_tibble(assignments)
#add grid information to arrests data
arrests_timessquare2015 <- arrests_timessquare2015 %>% 
  left_join(assignments, by = "ARREST_KEY")
#this plot shows how points are assigned to the grids
timessquare_map %>% ggmap() +
  geom_point(aes(x = Longitude, y = Latitude, color = grid), 
             data = arrests_timessquare2015) +
  geom_point(aes(x = Longitude, y = Latitude), data = lattice, col = "red", size = 5) +
  theme(legend.position = "none")
```

Next, let's aggregate the arrests to the grid level and display the results on a map.

```{r}
arrests_grid <- arrests_timessquare2015 %>% 
  count(grid, month(ARREST_DATE)) %>% 
  left_join(lattice)
#plot the aggregated results
timessquare_map %>% ggmap() +
  geom_point(aes(x = Longitude, y = Latitude, size = n), data = arrests_grid)
```

## Applying Lab 3.4 from the book to our aggregated data:

```{r}
library("ape")
library("dplyr")
library("FRK")
library("ggplot2")
library("gstat")
library("sp")
library("spacetime")
library("STRbook")
library("tidyr")
```

```{r}
G <- auto_basis(data = arrests_grid[,c("Longitude","Latitude")] %>%
                  SpatialPoints(),
                nres = 1,
                type = "Gaussian")
S <- eval_basis(basis = G,                                                # basis functions
                s = arrests_grid[,c("Longitude","Latitude")] %>%          
                  as.matrix()) %>%                                        # convert to matrix
  as.matrix()                                                             # convert to matrix
colnames(S) <- paste0("B", 1:ncol(S))                                     # assign column names
```

Attach the basis - function covariate information to the data frame containing the counts. 
```{r}
Grid_df <- cbind(arrests_grid,S) %>% rename("month" = "month(ARREST_DATE)")
Grid_df_rem <- Grid_df %>% select(-grid)
# View first five columns
Grid_df[1:3, 1:5]
```

Generalized Linear Model
```{r}
Arrest_GLM <- glm(n ~ (Longitude + Latitude + month)^2 + ., family = poisson("log"), data = Grid_df_rem)
Arrest_GLM$deviance / Arrest_GLM$df.residual
```
21.36 greater than 1, sign of over-dispersion

```{r}
Arrest_GLM$df.residual
```
```{r}
Arrest_GLM$deviance
```

```{r}
1 - pchisq(q = Arrest_GLM$deviance, df = Arrest_GLM$df.residual)
```
Reject the null hypothesis of no over-dispersion at the usual levels of significance.


Evaluate the basis functions at the prediction locations.
```{r}
S_pred <- eval_basis(basis = G,                                                    # basis funcs
                     s = lattice[,c("Longitude","Latitude")] %>%                   # pred locations
                       as.matrix()) %>%                                            # conv to matrix
  as.matrix()                                                                      # conv to matrix
colnames(S_pred) <- paste0("B", 1:ncol(S_pred))                                    # assign names
lattice <- cbind(lattice,S_pred)                                                   # attach to grid


```

Create matrix of prediction values using basis functions.
```{r}
Grid_df_rem_space <- Grid_df_rem %>% select(-Longitude,-Latitude)
Arrest_GLM_test <- glm(n ~ month^2 + ., family = poisson("log"), data = Grid_df_rem_space)
summary(Arrest_GLM_test)
```










